import random

class LLMFuzzer:
    """
    A prototype for an LLM-Guided Fuzzer.
    This core module manages test seeds and records security vulnerabilities.
    """
    
    def __init__(self):
        # Store test cases: { "seed_name": "input_string" }
        self.test_cases = {}
        # List to track detected vulnerabilities
        self.vulnerabilities = []

    def add_seed(self, name, data):
        """Adds a new input generated by the LLM to the corpus."""
        self.test_cases[name] = data
        print(f"[+] Seed added: {name}")

    def run_test(self, name):
        """
        Simulates sending the input to a target C binary.
        Evaluates if the system remains stable or crashes.
        """
        data = self.test_cases.get(name)
        if not data:
            return "ERROR: Seed not found"
        
        # Security Rule 1: Buffer Overflow (Input too long)
        if len(data) > 20:
            self.vulnerabilities.append({"type": "Buffer Overflow", "input": name})
            return "CRASH_DETECTED"
            
        # Security Rule 2: SQL Injection (Special character detected)
        if "$" in data:
            self.vulnerabilities.append({"type": "Injection Error", "input": name})
            return "CRASH_DETECTED"

        return "SUCCESS"

    def get_summary(self):
        """Returns a summary of the fuzzing campaign."""
        return {
            "total_tests": len(self.test_cases),
            "vulnerabilities_found": len(self.vulnerabilities)
        }

# --- EXAMPLE USAGE ---
if __name__ == "__main__":
    fuzzer = LLMFuzzer()
    
    # Simulating LLM-generated inputs
    fuzzer.add_seed("normal_user", "Annette")
    fuzzer.add_seed("overflow_attempt", "A" * 50)
    fuzzer.add_seed("injection_attempt", "admin$login")
    
    # Running the tests
    for seed in ["normal_user", "overflow_attempt", "injection_attempt"]:
        status = fuzzer.run_test(seed)
        print(f"Result for {seed}: {status}")
    
    print("\n--- FINAL REPORT ---")
    print(fuzzer.get_summary())

def generate_llm_seed(self, context_description):
        """
        Simulates an API call to an LLM (Claude/GPT) to get a clever test input.
        In CodeSignal, you'll often need to 'format' strings like this.
        """
        # Imagine the LLM returns a structured string based on our physics context
        simulated_response = f"ADVERSARIAL_INPUT_{random.randint(100, 999)}_EXP_ALPHA"
        
        # We store it using our existing method
        seed_name = f"llm_generated_{random.randint(1, 100)}"
        self.add_seed(seed_name, simulated_response)
        return seed_name
